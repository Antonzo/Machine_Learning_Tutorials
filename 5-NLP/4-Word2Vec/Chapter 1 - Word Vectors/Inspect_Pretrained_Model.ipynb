{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "------------------------\n",
    "In this notebook we'll play around with a pre-trained word model to look at its vocabulary and to try out some of the basic operations commonly performed on word vectors.\n",
    "\n",
    "We'll start by using the Python package `gensim` which implements all of the basic features we need like loading the model, accessing its vocabulary, and performing similarity lookups. Immediately after, though, we'll go \"under the covers\" and perform the same operations manually so you can see what's really going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "------------------\n",
    "\n",
    "* [Inspecting the Model](#inspect_model) \n",
    "    * [Model Vocabulary](#model_vocab)\n",
    "    * [Sample Vector](#sample_vector)\n",
    "* [Word Similarities with gensim](#word_sim_gensim)\n",
    "    * [Compare Two Words](#compare_two_gensim)\n",
    "    * [Find Most Similar](#find_sim_gensim)\n",
    "* [Word Similarities from Scratch](#word_sim_scratch)\n",
    "    * [Word-pair Similarity from Scratch](#word_pair_scratch)\n",
    "    * [Word Similarity Search from Scratch](#word_search_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Inspecting the Model <a name=\"inspect_model\"></a>\n",
    "-------------------------------------\n",
    "Along with the original word2vec papers, the authors [released](https://code.google.com/archive/p/word2vec/ \"Homepage for Google's Word2Vec code and pre-trained models\") a large Word2Vec model that they trained on roughly 100 billion words from a Google News dataset. It contains exactly 3 million words, and the word vectors have 300 features each. There are newer, presumably better, pre-trained models, but this is the original. \n",
    "\n",
    "Download the model file [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) (it's 3.39GB) and save it into a subdirectory: `./data/GoogleNews-vectors-negative300.bin` \n",
    "\n",
    "Once you've downloaded the model file, we'll use a helper function from `gensim` to load it as a `KeyedVector` class that provides a lot of convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "filepath = './data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Vocabulary<a name=\"model_vocab\"></a>\n",
    "I usually start by poking around the vocabulary of the model to get a feel for it. Let's print some random vocab words in three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 GoldDerby.com        frisky_flappers_dashing\n",
      "                          IAAO                   CardWizard_®\n",
      "                     Carlander                        ELKADER\n",
      "           sodium_metal_halide                        Olimpio\n",
      "                   Wolf_Cesman                          Inc.Â\n",
      "                     boondocks                         Sattui\n",
      "                CEREUS_Network              Kettle_Merger_Sub\n",
      "             HUMBERSIDE_Police                       Talgarth\n",
      "      unstatesmanlike_behavior                       Al_Nabat\n",
      "               Unsecured_Loans              shirts_emblazoned\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Retrieve the list of words in the vocabulary as 'vocab'\n",
    "vocab = list(model.vocab.keys())\n",
    "\n",
    "# Print 20 random words in two columns.\n",
    "for i in range(10):\n",
    "    # Choose and print two random words\n",
    "    print('%30s %30s' % (random.choice(vocab),\n",
    "                         random.choice(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Certainly a lot of non-sense in there! I've found this to be fairly typical of pre-trained word models. \n",
    "\n",
    "Let's look more explicitly at some different word types.\n",
    "\n",
    "I'm going to define a little helper function which takes lists of words and then reports which are found, checking both lower and upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocab(vocab, try_words):\n",
    "    print(\"%30s    %s\" % ('Word', 'Included'))    \n",
    "    print(\"%30s    %s\" % ('====', '========'))    \n",
    "    \n",
    "    for word in try_words:\n",
    "        print(\"%30s    %s\" % (word, str(word in model.vocab)))\n",
    "        # If the word isn't already lower case, try lower case as well.\n",
    "        if not word.lower() == word:\n",
    "            print(\"%30s    %s\" % (word.lower(), str(word.lower() in model.vocab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "                             a    False\n",
      "                           and    False\n",
      "                           the    True\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['a', 'and', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-word names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "               Abraham_Lincoln    True\n",
      "               abraham_lincoln    False\n",
      "                Michael_Jordan    False\n",
      "                michael_jordan    True\n",
      "                     Tom_Brady    True\n",
      "                     tom_brady    True\n",
      "                     Elon_Musk    True\n",
      "                     elon_musk    False\n",
      "                 United_States    True\n",
      "                 united_states    False\n",
      "      United_States_of_America    False\n",
      "      united_states_of_america    False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['Abraham_Lincoln',\n",
    "                    'Michael_Jordan',\n",
    "                    'Tom_Brady',\n",
    "                    'Elon_Musk',\n",
    "                    'United_States',\n",
    "                    'United_States_of_America',\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-word topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "              Computer_Science    True\n",
      "              computer_science    False\n",
      "                Global_Warming    True\n",
      "                global_warming    True\n",
      "                Foreign_Policy    False\n",
      "                foreign_policy    False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['Computer_Science',\n",
    "                    'Global_Warming',\n",
    "                    'Foreign_Policy',\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idioms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "                  couch_potato    True\n",
      "                  dime_a_dozen    False\n",
      "                  hit_the_sack    False\n",
      "                   cut_corners    False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['couch_potato',\n",
    "                    'dime_a_dozen',\n",
    "                    'hit_the_sack',\n",
    "                    'cut_corners',\n",
    "                    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Misspellings** (these are all misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "                    accomodate    True\n",
      "                      begining    True\n",
      "                      concious    True\n",
      "                    incidently    True\n",
      "                recomendations    True\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['accomodate',\n",
    "                    'begining',\n",
    "                    'concious',\n",
    "                    'incidently',\n",
    "                    'recomendations',\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "                         man`s    False\n",
      "                         man's    True\n",
      "                          it`s    False\n",
      "                          it's    True\n",
      "                        U.S.A.    True\n",
      "                        u.s.a.    False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['man`s',\n",
    "                    'man\\'s',\n",
    "                    'it`s',\n",
    "                    'it\\'s',\n",
    "                    'U.S.A.',\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Word    Included\n",
      "                          ====    ========\n",
      "                             1    True\n",
      "                           one    True\n",
      "                            12    False\n",
      "                        twelve    True\n",
      "                           100    False\n",
      "                   one_hundred    False\n"
     ]
    }
   ],
   "source": [
    "check_vocab(vocab, ['1',\n",
    "                    'one',\n",
    "                    '12',\n",
    "                    'twelve',\n",
    "                    '100',\n",
    "                    'one_hundred',\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Vector <a name=\"sample_vector\"></a>\n",
    "\n",
    "Let's take a look inside a single vector, for the word \"couch\". \n",
    "\n",
    "From the output we can see that the values appear to range within -1.0 to 1.0, and that the vector is dense rather than sparse (no features are zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: (300,)\n",
      "Sample values:  <0.2285, -0.2949, 0.0006, ..., 0.0854, -0.0018>\n",
      "Norm: 3.13\n",
      "Number of zeros: 0 of 300\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Let's peek at the word vector for the word 'couch'.\n",
    "vec = model.word_vec('couch')\n",
    "\n",
    "# Shape and sample values:\n",
    "print(\"Vector shape: \" + str(vec.shape))\n",
    "print(\"Sample values:  <%.4f, %.4f, %.4f, ..., %.4f, %.4f>\" % \n",
    "      (vec[0], vec[1], vec[2], vec[-2], vec[-1]))\n",
    "\n",
    "# What's the vector's magnitude?\n",
    "print(\"Norm: %.2f\" % np.linalg.norm(vec))\n",
    "\n",
    "# Are some values zero? How many?\n",
    "num_zero = len(vec) - np.count_nonzero(vec)\n",
    "print(\"Number of zeros: %d of %d\" % (num_zero, len(vec)))\n",
    "\n",
    "# Plot a histogram of the feature values to visualize their\n",
    "# distribution.\n",
    "ax = sns.distplot(vec, kde=False, rug=True)\n",
    "t = ax.set_title('Histogram of Feature Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarities with gensim <a name=\"word_sim_gensim\"></a>\n",
    "----------------------------------------------\n",
    "`gensim` includes convenience functions for computing a number of common word similarity operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Two Words<a name=\"compare_two_gensim\"></a>\n",
    "--------------------------------\n",
    "The `model` object has convenience functions for comparing two vectors. The below code shows that \"couch\" and \"book\" have a low similarity, while \"couch\" and \"sofa\" are very similar--as we would hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'couch' and 'book' is 0.12\n",
      "Cosine similarity between 'couch' and 'sofa' is 0.83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try comparing some specific words.\n",
    "# First, how similar are \"couch\" and \"book\"?\n",
    "score = model.similarity('couch', 'book')\n",
    "print(\"Cosine similarity between 'couch' and 'book' is %.2f\" % score)\n",
    "\n",
    "# How about \"couch\" and \"sofa\"?\n",
    "score = model.similarity('couch', 'sofa')\n",
    "print(\"Cosine similarity between 'couch' and 'sofa' is %.2f\\n\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Most Similar<a name=\"find_sim_gensim\"></a>\n",
    "----------------------------\n",
    "We can also find the most similar words in the vocabulary to \"couch\". \n",
    "\n",
    "The results look pretty sensible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most similar words to 'couch':\n",
      "                word    score\n",
      "                sofa    0.83\n",
      "            recliner    0.74\n",
      "             couches    0.70\n",
      "         comfy_couch    0.67\n",
      "               futon    0.65\n",
      "    al_Jabouri_slept    0.62\n",
      "            loveseat    0.62\n",
      "       beanbag_chair    0.62\n",
      "      recliner_chair    0.61\n",
      "              settee    0.61\n"
     ]
    }
   ],
   "source": [
    "# What are the 10 most similar words to \"couch\" in the vocabulary?\n",
    "results = model.most_similar(positive='couch', topn=10)\n",
    "\n",
    "# Print out the results.\n",
    "print(\"10 most similar words to 'couch':\")\n",
    "print(\"%20s    %s\" % ('word', 'score'))\n",
    "for (word, score) in results:\n",
    "    print(\"%20s    %.2f\" % (word, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun side note: What's with the \"al Jabouri slept\" result? I believe this is an artifact of the Google News dataset. This particular phrase comes from [this story](https://www.mercurynews.com/2010/12/29/three-suicide-bombers-used-to-kill-tenacious-iraqi-cop/) which mentions an officer named al-Jabouri sleeping on a couch! \n",
    "A problem with this Google News dataset is that news outlets all over will take articles (from Reuters, I think?) and just modify them slightly, so there are many near-duplicate news articles out there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity from Scratch<a name=\"word_sim_scratch\"></a>\n",
    "--------------------------------------\n",
    "It's great that gensim makes these operations easy for us, but to make sure we have a firm grasp on how they work, let's implement the above vector operations from scratch--just for educational purposes.\n",
    "\n",
    "(Side Note: If you want to keep playing with gensim a little more, there's nice documentation for the KeyedVectors class [here](https://radimrehurek.com/gensim/models/keyedvectors.html)).\n",
    "\n",
    "Let's start by pulling the word vectors matrix out of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector matrix is: (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Note - Older versions of gensim stored the vectors in `model.syn0` \n",
    "vecs = model.vectors\n",
    "\n",
    "print('Word vector matrix is: ' + str(vecs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-pair Similarity from Scratch<a name=\"word_pair_scratch\"></a>\n",
    "--------------------------\n",
    "Now let's pull out our specific word vectors manually by looking up their index from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm for \"couch\": 3.13\n",
      "Norm for \"sofa\": 3.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's look up the index of the vector for 'couch' and 'sofa'.\n",
    "w1 = model.vocab['couch'].index\n",
    "w2 = model.vocab['sofa'].index\n",
    "\n",
    "# Select the vectors using their row index.\n",
    "v1 = vecs[w1, :]\n",
    "v2 = vecs[w2, :]\n",
    "\n",
    "# Let's check out the norms of these two vectors.\n",
    "print('Norm for \"couch\": %.2f' % np.linalg.norm(v1))\n",
    "print('Norm for \"sofa\": %.2f' % np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Here's the formula for the cosine similarity of two vectors 'x' and 'y'. \n",
    "\n",
    "$ cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} $\n",
    "\n",
    "The formula is written as the dot-product of the vectors, divided by the product of their magnitudes. However, we can change the order and normalize the vectors first, then take their dot products. It's better to think of it in this order--we'll see why in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New norm of \"couch\": 1.00\n",
      "New norm of \"sofa\": 1.00\n",
      "\n",
      "Cosine similarity between \"couch\" and \"sofa\": 0.83\n",
      "    (gensim: 0.83)\n"
     ]
    }
   ],
   "source": [
    "# Normalize our vectors:\n",
    "v1_norm = v1 / np.linalg.norm(v1)\n",
    "v2_norm = v2 / np.linalg.norm(v2)\n",
    "\n",
    "# Let's double check the result:\n",
    "print('New norm of \"couch\": %.2f' % np.linalg.norm(v1_norm))\n",
    "print('New norm of \"sofa\": %.2f' % np.linalg.norm(v2_norm))\n",
    "\n",
    "# Now we can take the dot-product of the normalized vectors:\n",
    "cos_sim = np.dot(v1_norm, v2_norm)\n",
    "\n",
    "print('\\nCosine similarity between \"couch\" and \"sofa\": %.2f' % cos_sim)\n",
    "\n",
    "# Also show the gensim results as a sanity-check\n",
    "print('    (gensim: %.2f)' % model.similarity('couch', 'sofa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity Search from Scratch<a name=\"word_search_scratch\"></a>\n",
    "----------------------------\n",
    "Now let's try searching the vocabulary for the top-10 most similar words to \"couch\".\n",
    "\n",
    "We could iterate over all 3M words in the vocabulary, calculating the cosine similarity as we did above, and then sorting them. This is brutally slow, though! As you may know, it's much more efficient to perform vector-matrix operations. We'll multiply the vector against the whole matrix, and the processor will be able to use efficient linear algebra routines and SIMD instructions to speed up this heavy compute task.\n",
    "\n",
    "Here is where it's going to help us to change up the order of operations. If we simply normalize the entire word vector matrix as pre-processing step, then we never have to worry about the normalization step again!\n",
    "\n",
    "We start by calculating the norms for all 3M vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating vector norms, this can be slow...\n",
      "\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Calculating vector norms, this can be slow...\\n\")\n",
    "\n",
    "# First, numpy can calculate the norms of all of our vectors.\n",
    "# We specify that we want the norms calculated along the first axis, \n",
    "# since these are row vectors.\n",
    "norms = np.linalg.norm(vecs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "Now we divide the vectors by their norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vecs: (3000000, 300)\n",
      "Shape of norms: (3000000,)\n",
      "\n",
      "Normalizing all vectors, this can be slow...\n",
      "\n",
      "New norm of first vector: 1.00 \n",
      "\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Shape of vecs: \" + str(vecs.shape))\n",
    "print(\"Shape of norms: \" + str(norms.shape))\n",
    "\n",
    "# Add a second dimension to norms, so that it's 3M x 1.\n",
    "norms = norms.reshape(len(norms), 1) \n",
    "\n",
    "print(\"\\nNormalizing all vectors, this can be slow...\")\n",
    "\n",
    "# Vecs is [3M x 300] and norms is [3M x 1]. Performing division \n",
    "# will result in each row of 'vecs' being divided by the scalar \n",
    "# in the corresponding row of 'norms'.\n",
    "vecs_norm = vecs / norms\n",
    "\n",
    "# Sanity check...\n",
    "print(\"\\nNew norm of first vector: %.2f \\n\" % \n",
    "      (np.linalg.norm(vecs_norm[0, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Now that we have the normalized vectors, we can calculate the cosine similarities for 'couch' and all 3M vocabulary words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating all word similarities...\n",
      "\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Look up the index of the vector for 'couch'.\n",
    "w_i = model.vocab['couch'].index\n",
    "\n",
    "# Select the *normalized* vector using the row index.\n",
    "v_norm = vecs_norm[w_i, :]\n",
    "\n",
    "# For our matrix-vector multiplication, we need v_norm\n",
    "# as [300 x 1]\n",
    "v_norm = v_norm.reshape(len(v_norm), 1)\n",
    "\n",
    "print('Calculating all word similarities...\\n')\n",
    "\n",
    "# Perform the matrix-vector multiplication.\n",
    "#   vecs_norm   *   v_norm     =  all_sims\n",
    "#   [3M x 300]  *   [300 x 1]  =  [3M x 1]\n",
    "all_sims = vecs_norm.dot(v_norm)\n",
    "\n",
    "# Remove the extra dimension from the similarity values.\n",
    "all_sims = all_sims.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "The final step is simply to sort the results and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting similarities...\n",
      "\n",
      "Top 10 most similar words to 'couch':\n",
      "               couch   1.00\n",
      "                sofa   0.83\n",
      "            recliner   0.74\n",
      "             couches   0.70\n",
      "         comfy_couch   0.67\n",
      "               futon   0.65\n",
      "    al_Jabouri_slept   0.62\n",
      "            loveseat   0.62\n",
      "       beanbag_chair   0.62\n",
      "      recliner_chair   0.61\n",
      "\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The gensim class contains a list of all words in the vocabulary.\n",
    "# We'll need this list in order to map back from row indeces to\n",
    "# their words.\n",
    "vocab_words = model.index2word\n",
    "\n",
    "# Turn the similarities vector into a list of tuples in the form\n",
    "#   (index, similarity)\n",
    "# e.g., \n",
    "#  [(0, 0.03), (1, 0.20), (2, 0.08), ...]\n",
    "results = enumerate(all_sims)\n",
    "\n",
    "print(\"Sorting similarities...\\n\")\n",
    "\n",
    "# Now sort the list of tuples by the similarity value.\n",
    "# Sort descending, with highest similarity first.\n",
    "results = sorted(results, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most similar words to 'couch':\")\n",
    "\n",
    "# Display the top 10 results and their similarity.\n",
    "for i in range(10):\n",
    "    # Get the word index for result 'i'.\n",
    "    word_index = results[i][0]\n",
    "    \n",
    "    # Lookup the word.\n",
    "    word = vocab_words[word_index]\n",
    "    \n",
    "    # Print the word and its similarity value.\n",
    "    print('%20s   %.2f' % (word, results[i][1]))\n",
    "    \n",
    "print('')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "A small misconception around cosine similarity is that it is always positive, but this is not the case for vectors which contain negative feature values (such as word vectors). The cosine of 180 degrees is -1, so two vectors pointing in opposite directions will have cosine similarity -1.\n",
    "\n",
    "Just for fun, what does the model think are the *least* similar words to \"couch\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 least similar words to 'couch':\n",
      "    T.Stewart_##-###   -0.28\n",
      "         de_Securite   -0.27\n",
      "        Nasdaq_AKZOY   -0.27\n",
      "        Butch_Alinea   -0.26\n",
      "Asset_Management_NGAM   -0.26\n",
      "               etwcf   -0.25\n",
      "         Stanis_³_aw   -0.25\n",
      "         Algiers_AAI   -0.25\n",
      "     K.Kahne_###-###   -0.24\n",
      "Basmati_Growers_Association   -0.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 least similar words to 'couch':\")\n",
    "\n",
    "# Display the last 10 results and their similarity values.\n",
    "# (Python let's us iterate backwards through a list with\n",
    "# negative indeces).\n",
    "for i in range(-1, -11, -1):\n",
    "    # Get the word index for result 'i'.\n",
    "    word_index = results[i][0]\n",
    "    \n",
    "    # Lookup the word.\n",
    "    word = vocab_words[word_index]\n",
    "    \n",
    "    # Print the word and its similarity value.\n",
    "    print('%20s   %.2f' % (word, results[i][1]))\n",
    "    \n",
    "print('')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know about you, but when I think about the opposite of \"couch\" the first thing that comes to mind is definitely the *Basmati Growers Association*! :)\n",
    "\n",
    "----------\n",
    "\n",
    "Another \"just for fun\" exercise, here's a faster way to sort the results.\n",
    "Numpy's `argsort` function returns just the sorted indeces, which is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sofa   0.83\n",
      "            recliner   0.74\n",
      "             couches   0.70\n",
      "         comfy_couch   0.67\n",
      "               futon   0.65\n",
      "    al_Jabouri_slept   0.62\n",
      "            loveseat   0.62\n",
      "       beanbag_chair   0.62\n",
      "      recliner_chair   0.61\n",
      "              settee   0.61\n",
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Sort the similarities but return the sorted *indeces*.\n",
    "results2 = np.argsort(all_sims, axis=0)\n",
    "\n",
    "# For the top 10 results...\n",
    "for i in range(-2, -12, -1):\n",
    "    # Get the word index for result 'i' (in reverse order).\n",
    "    word_index = results2[i]\n",
    "    \n",
    "    # Lookup the word.\n",
    "    word = vocab_words[word_index]\n",
    "    \n",
    "    # Lookup the calculated similarity value.\n",
    "    sim = all_sims[word_index]\n",
    "    \n",
    "    # Print the word and its similarity value.\n",
    "    print('%20s   %.2f' % (word, sim))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
