{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T19:54:41.280498Z",
     "start_time": "2019-03-29T19:54:41.274176Z"
    }
   },
   "source": [
    "<center><h1>Bag-Of-Words Embedding</center></h1>\n",
    "<center> Exploring TF and TF-IDF methods </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T19:09:43.094257Z",
     "start_time": "2019-03-31T19:09:43.084919Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw, wordnet as wn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T20:19:01.416112Z",
     "start_time": "2019-03-29T20:19:01.412647Z"
    }
   },
   "source": [
    "# BOW from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use South Park scripts as our database for messages : https://github.com/BobAdamsEE/SouthParkData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, split per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:20:54.372456Z",
     "start_time": "2019-03-31T18:20:54.304433Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
    "    \"\"\"\n",
    "    Returns a normalized, lemmatized list of tokens from a document by\n",
    "    applying segmentation (breaking into sentences), then word/punctuation\n",
    "    tokenization, and finally part of speech tagging. It uses the part of\n",
    "    speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "    version of all the words, removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def lemmatize(token, tag):\n",
    "        \"\"\"\n",
    "        Converts the tag to a WordNet POS tag, then uses that\n",
    "        tag to perform an accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return WordNetLemmatizer().lemmatize(token, tag)\n",
    "\n",
    "    def vectorize(doc, max_features, max_sentence_len):\n",
    "        \"\"\"\n",
    "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(doc)\n",
    "        doc = tokenizer.texts_to_sequences(doc)\n",
    "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
    "        return np.squeeze(doc_pad), tokenizer.word_index\n",
    "\n",
    "    # Clean the text using a few regular expressions\n",
    "    document = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", document)\n",
    "    document = re.sub(r\"what's\", \"what is \", document)\n",
    "    document = re.sub(r\"\\'\", \" \", document)\n",
    "    document = re.sub(r\"@\", \" \", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
    "    document = re.sub(r\"can't\", \"cannot \", document)\n",
    "    document = re.sub(r\"n't\", \" not \", document)\n",
    "    document = re.sub(r\"i'm\", \"i am \", document)\n",
    "    document = re.sub(r\"\\'re\", \" are \", document)\n",
    "    document = re.sub(r\"\\'d\", \" would \", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
    "    document = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", document)\n",
    "    document = document.replace(\"\\n\", \" \")\n",
    "    \n",
    "    cleaned_document = []\n",
    "    vocab = []\n",
    "    \n",
    "    # Break the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        lemmatized_tokens = []\n",
    "\n",
    "        # Break the sentence into part of speech tagged tokens\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\n",
    "            # Apply preprocessing to the tokens\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation ignore token and continue\n",
    "            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or \n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "            vocab.append(lemma)\n",
    "            \n",
    "        cleaned_document.append(lemmatized_tokens)\n",
    "    \n",
    "    vocab = sorted(list(set(vocab)))\n",
    "    \n",
    "    return cleaned_document, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, split per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:30:02.607801Z",
     "start_time": "2019-03-31T18:30:02.529088Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
    "    \"\"\"\n",
    "    Returns a normalized, lemmatized list of tokens from a document by\n",
    "    applying segmentation (breaking into sentences), then word/punctuation\n",
    "    tokenization, and finally part of speech tagging. It uses the part of\n",
    "    speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "    version of all the words, removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def lemmatize(token, tag):\n",
    "        \"\"\"\n",
    "        Converts the tag to a WordNet POS tag, then uses that\n",
    "        tag to perform an accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return WordNetLemmatizer().lemmatize(token, tag)\n",
    "\n",
    "    def vectorize(doc, max_features, max_sentence_len):\n",
    "        \"\"\"\n",
    "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(doc)\n",
    "        doc = tokenizer.texts_to_sequences(doc)\n",
    "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
    "        return np.squeeze(doc_pad), tokenizer.word_index\n",
    "    \n",
    "    cleaned_document = []\n",
    "    vocab = []\n",
    "    \n",
    "    # Break the document into sentences\n",
    "    for sent in document:\n",
    "        \n",
    "        # Clean the text using a few regular expressions\n",
    "        sent = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", sent)\n",
    "        sent = re.sub(r\"what's\", \"what is \", sent)\n",
    "        sent = re.sub(r\"\\'\", \" \", sent)\n",
    "        sent = re.sub(r\"@\", \" \", sent)\n",
    "        sent = re.sub(r\"\\'ve\", \" have \", sent)\n",
    "        sent = re.sub(r\"can't\", \"cannot \", sent)\n",
    "        sent = re.sub(r\"n't\", \" not \", sent)\n",
    "        sent = re.sub(r\"i'm\", \"i am \", sent)\n",
    "        sent = re.sub(r\"\\'re\", \" are \", sent)\n",
    "        sent = re.sub(r\"\\'d\", \" would \", sent)\n",
    "        sent = re.sub(r\"\\'ll\", \" will \", sent)\n",
    "        sent = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", sent)\n",
    "        sent = sent.replace(\"\\n\", \" \")\n",
    "    \n",
    "        lemmatized_tokens = []\n",
    "\n",
    "        # Break the sentence into part of speech tagged tokens\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\n",
    "            # Apply preprocessing to the tokens\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation ignore token and continue\n",
    "            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or \n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "            vocab.append(lemma)\n",
    "            \n",
    "        cleaned_document.append(lemmatized_tokens)\n",
    "    \n",
    "    vocab = sorted(list(set(vocab)))\n",
    "    \n",
    "    return cleaned_document, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:30:58.190171Z",
     "start_time": "2019-03-31T18:30:58.033573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           You guys, you guys! Chef is going away. \\n\n",
       "1                          Going away? For how long?\\n\n",
       "2                                           Forever.\\n\n",
       "3                                    I'm sorry boys.\\n\n",
       "4    Chef said he's been bored, so he joining a gro...\n",
       "Name: Line, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('All-Seasons.csv')['Line'][:1000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:31:00.352870Z",
     "start_time": "2019-03-31T18:30:59.193141Z"
    }
   },
   "outputs": [],
   "source": [
    "df, vocab = preprocess(list(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:31:09.868699Z",
     "start_time": "2019-03-31T18:31:09.862093Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bow(allsentences):    \n",
    "    bag_vector = np.zeros((len(allsentences), len(vocab)))\n",
    "    for j in range(len(allsentences)):\n",
    "        for w in allsentences[j]:\n",
    "            for i,word in enumerate(vocab):\n",
    "                if word == w: \n",
    "                    bag_vector[j,i] += 1\n",
    "    return bag_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:31:11.876784Z",
     "start_time": "2019-03-31T18:31:10.570826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1589)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = generate_bow(df)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T20:19:01.416112Z",
     "start_time": "2019-03-29T20:19:01.412647Z"
    }
   },
   "source": [
    "# BOW in Sk-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:44:25.437971Z",
     "start_time": "2019-03-31T18:44:25.279013Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('All-Seasons.csv')['Line'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:44:25.505702Z",
     "start_time": "2019-03-31T18:44:25.455983Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
    "    \"\"\"\n",
    "    Returns a normalized, lemmatized list of tokens from a document by\n",
    "    applying segmentation (breaking into sentences), then word/punctuation\n",
    "    tokenization, and finally part of speech tagging. It uses the part of\n",
    "    speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "    version of all the words, removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def lemmatize(token, tag):\n",
    "        \"\"\"\n",
    "        Converts the tag to a WordNet POS tag, then uses that\n",
    "        tag to perform an accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return WordNetLemmatizer().lemmatize(token, tag)\n",
    "\n",
    "    def vectorize(doc, max_features, max_sentence_len):\n",
    "        \"\"\"\n",
    "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(doc)\n",
    "        doc = tokenizer.texts_to_sequences(doc)\n",
    "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
    "        return np.squeeze(doc_pad), tokenizer.word_index\n",
    "\n",
    "    cleaned_document = []\n",
    "    vocab = []\n",
    "    \n",
    "    # Break the document into sentences\n",
    "    for sent in document:\n",
    "        \n",
    "        lemmatized_tokens = []\n",
    "        # Clean the text using a few regular expressions\n",
    "        sent = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", sent)\n",
    "        sent = re.sub(r\"what's\", \"what is \", sent)\n",
    "        sent = re.sub(r\"\\'\", \" \", sent)\n",
    "        sent = re.sub(r\"@\", \" \", sent)\n",
    "        sent = re.sub(r\"\\'ve\", \" have \", sent)\n",
    "        sent = re.sub(r\"can't\", \"cannot \", sent)\n",
    "        sent = re.sub(r\"n't\", \" not \", sent)\n",
    "        sent = re.sub(r\"i'm\", \"i am \", sent)\n",
    "        sent = re.sub(r\"\\'re\", \" are \", sent)\n",
    "        sent = re.sub(r\"\\'d\", \" would \", sent)\n",
    "        sent = re.sub(r\"\\'ll\", \" will \", sent)\n",
    "        sent = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", sent)\n",
    "        sent = sent.replace(\"\\n\", \" \")\n",
    "\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\n",
    "            # Apply preprocessing to the tokens\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation ignore token and continue\n",
    "            if all(char in set(string.punctuation) for char in token): #token in set(sw.words('english')) or \n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "            \n",
    "        cleaned_document.append(' '.join(lemmatized_tokens))\n",
    "    \n",
    "    return cleaned_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:44:26.713754Z",
     "start_time": "2019-03-31T18:44:25.634827Z"
    }
   },
   "outputs": [],
   "source": [
    "df = preprocess(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:47:57.714041Z",
     "start_time": "2019-03-31T18:47:57.664592Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T18:48:02.549377Z",
     "start_time": "2019-03-31T18:48:02.544564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1568)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T19:09:23.042086Z",
     "start_time": "2019-03-31T19:09:23.038900Z"
    }
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T19:10:04.780973Z",
     "start_time": "2019-03-31T19:10:04.735911Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
